\section{论文工作是否按预期进行、目前已完成的研究工作及结果}
\subsection{论文工作是否按预期进行}
论文工作按照预期进行: \par
2022.11 - 2022.12: 代码框架搭建完成, 基于相机的SDK开发相机取图进程，实现检测部分的传统算法。\par
2023.01 - 2023.02：实现检测部分的深度学习部分：数据集制作、网络训练、网络部署。\par
2023.02 - 2023.03：实现基于卡尔曼滤波器的追踪和预测部分。\par
\subsection{目前已完成的研究工作及结果}
\subsubsection{检测算法中的传统视觉部分}
传统视觉算法负责的内容是提取出感兴趣的区域，即装甲板区域。总体流程是：二值化图像、提取轮廓、灯条判别、灯条匹配装甲板。
首先为了能去提取装甲板的灯条信息需要先将得到图像进行二值化的处理，因为装甲板左右两侧的灯条的亮度是远高于背景环境的，
所以通过设定一个阈值二值化图像便可以很好的将装甲板的灯条提取出来，但是在测试中发现由于装甲板所处的光照环境不同，
对于二值化阈值的选取有着一定的难度，为了让二值化提取能够更好的适应不同的环境我尝试了以下几种方法：\par
1. 使用全局自适应二值化。\par
2. 使用红蓝通道相减的方法对相减之后的图像进行二值化。\par
3. 使用红/蓝通道进行二值化，同时将原图的灰度图二值化，最后将两张图按位与，得到所需的二值化图片。\par

针对以上三种二值化方案我分别进行了测试，首先第一种全局自适应二值化可以很方便的使用OpenCV内置的api实现，不过通过对大量场景下的测试发现由于部分情况下灯条可能与环境中某一处的灯光所重合，
由于二者亮度相差不大，所以自适应二值化会将二者一同处理，造成最终得到的灯条轮廓不闭合，对之后的轮廓筛选造成影响，所以这个方法的适应性一般。\par

第二种方法是基于OpenCV在图像处理中的特点而实现，OpenCV中如果两张图片相减后得到的像素点的值是小于0的那么会直接让它等于0，
而所需要识别的装甲板的灯条只有红蓝两种形式，若将红蓝两通道相减则可以很好的将灯条信息提取出来，但是由于相机参数的影响，
有时会使得灯条的中部为白色，这样的话对应位置红蓝两通道的值相差便不大，
相减之后得到的值几乎为0，二值化之后得到的灯条图像就不再是一个类似矩形的形状，而是一个圆环状，这种情况下也是可能出现轮廓不闭合的问题，导致之后的轮廓提取出现问题。\par

第三种方案通过众多的测试发现在多种情况下都能比较不错的实现对于灯条的提取，
通过分析可以发现由于使用的是灰度图像所以对于场地颜色还有亮度的适应性更强，结合对于颜色通道的二值化图像可以较好的得到最终所需的二值化图像。\par

经过了二值化之后所有亮度高于灯条或者与灯条亮度一致的区域都会被保留下来，需要通过筛选将一些明显不符合灯条特征的区域筛去。\par

首先找出图像中所有的轮廓，然后通过OpenCV中拟合矩形的两种的方式(minAreaRect、fitEllipse)获取所有轮廓的外接矩形，
获取外接矩形后便拥有了它们的长宽以及角度信息，这样便可以利用轮廓的长宽比以及它们的角度进行一个初步的筛选，将满足条件的装甲板存储起来进行下一步的判断。\par

在得到了所有的可能的灯条后需要将灯条两两匹配，找到属于每个装甲板的那组灯条，装甲板为一个很标准的矩形，尽管视野中装甲板的距离、角度等都会有所不同，
但是两根灯条之间的相关参数都还是处于一定范围之内的，通过判断两根灯条的x、y方向上的距离、长宽比、倾斜角度、角度差
等信息得到满足装甲板条件的一对灯条，得到了这对灯条的信息后其实也就得到了对应装甲板的各类信息，便于后续计算的使用。\par
如下图所示，传统图像处理算法提取中多个可能存在装甲板的ROI。

\subsubsection{检测算法中的深度学习部分}

为了防止出现误识别的情况，提取到“装甲板”对应的ROI区域后需要将它们放入分类器去进行分类，此外对于不同的兵种（装甲板对应不同的数字）击打的优先级是不同的，因此对于装甲板类别的判断是非常重要的。\par
目前对于图像分类的方式有许多种，比如基于卷积神经网络方案和传统机器学习（如SVM）等。我们要做的是对装甲板分类，装甲板的特征比较简单但是类别较多，同时还需要具备对于负样本的判断能力，
基于这几个特点我们选用了带残差层的全连接神经网络。全连接的神经网络虽然形式简单同时也能获得一个较高的准确度，
但是全连接层巨大的参数量会对运行时间造成很大的影响，因此我们考虑使用全局平均池化去解决参数量大的问题，
可使用全局平均池化虽然减少了参数量却丢失了许多信息，测试发现模型收敛后很难达到一个较高的准确度。
对于此问题我们所采用的解决方案是在两层卷积层之后引入一个残差块，将前两层卷积所提取到的特征信息与第三层卷积层的计算结果融合起来，
然后再去使用全局平均池化，经过测试发现最终它所能达到的准确甚至是可以略微超过单纯的全连接神经网络，具体网络结构如下：\par
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{classify_network_structure.png} 
    \caption{分类器网络结构} 
\end{figure}

下图展示训练时的准确率、LOSS值的变化以及ROC曲线绘制。
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{train_accuracy.png} 
    \caption{训练时准确率变化} 
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{train_loss.png} 
    \caption{训练时LOSS值变化} 
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{train_ROC.png} 
    \caption{ROC曲线} 
\end{figure}

下图展示训练后的神经网络在面对新数据时的表现。
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{classify_demo.png} 
    \caption{分类器测试结果} 
\end{figure}

下图为，经过分类器判别之后得到的装甲板。
\subsubsection{卡尔曼滤波器与运动预测算法}

卡尔曼滤波器是用来对数据滤波的，与运动预测没有直接关系。
运动预测本质上是对运动物体的建模，比如，通过观测知道了当前物体的位置，
并且通过历史的观测数据得到了物体的运动信息（如速度、加速度等），
那么就可以预测未来在$t$时刻物体的位置，以在一维$x$方向匀速运动的物体为例 \par
$x(t) = x_0 + v_x*t$  \par
所谓的对运动物体的建模其实就是假设一个运动模型，通过数据去拟合参数，
比如，匀速运动模型就是通过一堆带着时间信息的坐标点去拟合速度。\par
接下来我们考虑坐标系的选择。通过PnP算法或者是其他算法解算的目标位置（实际上得到的是位置和姿态，
但是运动预测不需要目标的姿态，因此下文只考虑目标的位置）的坐标系是相机坐标系，但是，
云台运动时相机坐标系的位姿也会有变化，观测的相对于大地静止的目标也会发生变化，这样不利于建立运动模型。所以，
需要将目标的位置坐标转换到惯性坐标系下，这样目标点的位置不随着云台运动而运动，从而可以建立准确的运动学模型。\par
由于设备采用的是单目相机，得到某点的信息只有其在$x$和$y$方向的像素坐标信息，
其实得到是目标相对于相机坐标系的$yaw$和$pitch$轴角度信息，
也就是说，相机成像的物理模型使得在观测物体的位置时天生选择的是球坐标系。
至于深度信息，也就是半径，单目相机在没有先验知识的情况下是无法得出的。
但是装甲板的实际尺寸大小是已知的(先验)，由透镜成像的物理模型可得：\par

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{camera_model.png} 
    \caption{透镜成像模型} 
\end{figure}    

$d = \frac{fh}{y}$

这样，通过计算灯条的像素高度、事先标定好的相机内参中的$f_y$和先验装甲版实际高度就可以计算出目标相对于相机的实际距离。
注意，上面的$y$的单位不是像素，而是$mm$、$m$这样的单位，
而实际从图像上读的是像素值，因此实际的计算公式如下（看不懂的可以看一下相机的成像模型以及坐标系转换关系）：
$d=\frac{fh}{y_{像素}}=\frac{fh}{y*dy}=\frac{f_yh}{y}$
这样，就得到了物体在相机坐标系下的位置描述$(yaw,pitch, distance)$。
当然，可以把它再转换到笛卡尔坐标系$(x,y,z)$
$x = distance*cos(pitch)*sin(yaw)$
$y = distance*cos(pitch)*cos(yaw)$
$z = distacne * sin(pitch)$
不过需要注意的是，转换之后的$x,y,z$之间存在耦合关系（例如：x、y都被distancce影响，当distance发生改变时，x、y同时都会改变）；
但是$yaw,pitch,distance$之间则相互独立。\par

从计算深度距离$distance$可以看出其与物体在，这里最大不确定的因素就是顶像素点的位置，
提取像素点不可能完全的准确。采用图像二值化的方式，提取的轮廓边缘的像素值一定是在阈值附近的，
有时高于阈值，有时低于阈值，影响就是结算出来的距离一直在波动；且随着距离的增加，构成灯条的像素点数减少，像素点的波动对于测距影响增大，影响就是距离越远，测距误差越大。
因为上述所说的噪声不可避免，所以采用卡尔曼滤波器得到距离的最优估计值。\par


要使用卡尔曼滤波器，我们首先需要确定运动模型。对于运动模型，首先需要明确的是运动模型实际上是无法用数学公式描述的，
即使能够用数学公式描述对于而言也是不可知的。 所以只能假设它是匀速运动模型或者是匀加速运动模型或是其他运动模型。
对于Robomaster比赛来说，匀速运动模型能够应对大多数场合。然而，机器人加减速频繁，那么选择匀加速模型是不是更准确呢？ 
经过测试发现，引入加速度后，从原先只需要拟合一个参数变成了拟合两个参数，容易造成拟合结果的不准确。且容易产生过拟合现象。
如下图所示，高阶模型固然对已有的点拟合的准确，但从图中明显可以看出如果新加一个点，必然与拟合的曲线相差甚大。
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{overfit_demo.png} 
    \caption{过拟合现象可视化} 
\end{figure} 

本段介绍卡尔曼滤波器的核心思想。考虑这样一个情形，目标是知道在时刻$t$目标的位置。首先可以通过观测得到，
然而观测会有噪声，一个减小误差的手段是选取一段时间区间内的平均观测值作为目标在$t$时刻的位置，
平均值会降低噪声的影响，且区间越大，对于噪声的抑制效果越明显，但是他的缺点是滞后。\par

卡尔曼给出的解决思路是：在状态转移模型已知的情况下，根据$t-1$时刻的状态推断$t$时刻的状态$\hat{x}_k^{-}$(先验状态估计，也叫预测值)；
再将预测值与在$t$时刻的观测值$z_k$两者融合，得到$\hat{x}_k$(后验状态估计，也叫估计值，也是最优估计值)，其中$z_k=Hx_k+v_k$。\par

根据上述，有（注意，这里目前只讨论一维的情况）： \par
$\hat{x}_k = k*\hat{x}_k^{-} + (1-k)*x_k$

最简单的想法是可以平均一下对吧（$k=0.5$），然而这种思路没有充分考虑两种数据来源的不确定度。
预测值有噪声，观测值有噪声，卡尔曼给出的优化目标是让这个融合的值的噪声的方差最小。
即，系统的白噪声激励和测量噪声并不是需要滤除的对象，它们的统计特性是估计过程中需要利用的信息，这一点与最小二乘和平均值法是有区别的。\par

卡尔曼基本模型如下
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{overfit_demo.png} 
    \caption{过拟合现象可视化} 
\end{figure} 


\section{后期拟完成的研究工作及进度安排}
\subsection{后期拟完成的研究工作}
1. 完成单目相机测距算法。\par
2. 完成受空气阻力的弹道迭代计算。\par
3. 充分测试系统的鲁棒性，如识别算法在不同光照环境下的表现效果，预测算法在角度跨圈时处理是否得当，滤波算法是否能够很好的应对不同的噪声。\par
4. 与控制系统联合调试，测试实际效果。\par
\subsection{后期进度安排}
以上（后期拟完成的研究工作）均在2023.04月份完成。
\section{存在的问题与困难}
1. 赛场环境非常复杂，而图像预处理的二值化操作严重依赖于阈值的选择。相同曝光时间下，因环境亮度不同， 二值化阈值也不同，
需要设计硬件自动曝光与软件自动曝光算法，使得在不同光照环境下得到的图像亮度基本保持一致。\par

2. 数字识别在低亮度环境下表现效果不好。因为要追求高帧率的目标检测，假设目标帧率为 150fps，
则最大曝光时间不超过6.7ms，在弱光照环境下成像较暗，卷积分类网络表现效果不好。针对此问题，计划通过两种方式解决:
- 增加在此光照环境下的数据集，让神经网络多学习这一场景。
- 由于数字图案的背景都是黑色的，而数字本身是白色的，通过大津二值化算法实现数字与背景的图像分割，让数字图案更加清晰，数字特征更加明显。\par
\section{论文按时完成的可能性}
论文能够保证按时完成，主要技术问题都得到解决，剩下的只是一些小的细节和优化。\par
\section{参考文献}
\bibliographystyle{hithesis}
\bibliography{reference}

% Local Variables:
% TeX-master: "../mainart"
% TeX-engine: xetex
% End: